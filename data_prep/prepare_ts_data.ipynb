{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "\n",
    "# set the seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_task_description_en = \"\"\"### Human: Please rewrite the following complex sentence in order to make it easier to understand.\n",
    "You can do so by replacing complex words with simpler synonyms (i.e. paraphrasing), deleting unimportant information (i.e. compression), and/or splitting a long complex sentence into several simpler ones.\n",
    "The final simplified sentence needs to be grammatical, fluent, and retain the main ideas of its original counterpart without altering its meaning.\n",
    "\n",
    "Complex sentence: {complex_sentence}\n",
    "\n",
    "### Assistant: \"\"\"\n",
    "\n",
    "ts_task_description_de = \"\"\"### Human: Bitte schreibe den folgenden komplexen Satz so um, dass er leichter zu verstehen ist.\n",
    "Du kannst dies tun, indem du komplexe Wörter durch einfachere Synonyme ersetzt (d.h. Paraphrasierung), unwichtige Informationen streichst (d.h. Komprimierung) und/oder einen langen komplexen Satz in mehrere einfachere Sätze aufteilst.\n",
    "Der endgültige vereinfachte Satz muss grammatikalisch einwandfrei und flüssig sein und die Hauptgedanken des ursprünglichen Satzes beibehalten, ohne dessen Bedeutung zu verändern.\n",
    "\n",
    "Komplexer Satz: {complex_sentence}\n",
    "\n",
    "### Assistant: \"\"\"\n",
    "\n",
    "ts_task_description_fr = \"\"\"### Human: Veuillez réécrire la phrase complexe suivante afin de la rendre plus facile à comprendre.\n",
    "Vous pouvez le faire en remplaçant les mots complexes par des synonymes plus simples (paraphrase), en supprimant les informations non importantes (compression) et/ou en divisant une longue phrase complexe en plusieurs phrases plus simples.\n",
    "La phrase simplifiée finale doit être grammaticale, fluide et conserver les idées principales de sa contrepartie originale sans en modifier le sens.\n",
    "\n",
    "Phrase complexe: {complex_sentence}\n",
    "\n",
    "### Assistant: \"\"\"\n",
    "\n",
    "ts_task_description_it = \"\"\"### Human: Si prega di riscrivere la seguente frase complessa per renderla più comprensibile.\n",
    "Per farlo, si possono sostituire parole complesse con sinonimi più semplici (parafrasare), eliminare informazioni non importanti (compressione) e/o suddividere una frase lunga e complessa in molteplici frasi più semplici.\n",
    "La frase semplificata finale deve essere grammaticalmente scorrevole e mantenere le nozioni principali della sua controparte originale senza alterarne il significato.\n",
    "\n",
    "Frase complessa: {complex_sentence}\n",
    "\n",
    "### Assistant: \"\"\"\n",
    "\n",
    "# to check\n",
    "ts_task_description_ru = \"\"\"### Human: Перепиши следующее сложное предложение, чтобы сделать его более понятным.\n",
    "Для этого можно заменить сложные слова более простыми синонимами (перефразирование), удалить несущественную информацию (сжатие) и/или разбить длинное сложное предложение на несколько более простых.\n",
    "Итоговое упрощенное предложение должно быть грамматически правильным, беглым и сохранять основные идеи своего оригинала без изменения его смысла.\n",
    "\n",
    "Сложное предложение: {complex_sentence}\n",
    "\n",
    "### Assistant: \"\"\"\n",
    "\n",
    "# to check\n",
    "ts_task_description_pt = \"\"\"### Human: Reescreva a frase complexa a seguir para facilitar a compreensão.\n",
    "Você pode fazer isso substituindo palavras complexas por sinônimos mais simples (ou seja, parafraseando), excluindo informações sem importância (ou seja, comprimindo) e/ou dividindo uma frase longa e complexa em várias frases mais simples.\n",
    "A frase final simplificada precisa ser gramatical, fluente e manter as ideias principais de sua contraparte original sem alterar seu significado.\n",
    "\n",
    "Frase complexa: {complex_sentence}\n",
    "\n",
    "### Assistant: \"\"\"\n",
    "\n",
    "ts_task_description_ja = \"\"\"### Human: 次の複雑な文章を、より理解しやすく書き直してください。\n",
    "複雑な単語をより簡単な同義語に置き換える（パラフレーズ）、重要でない情報を削除する（圧縮）、または長く複雑な文をいくつかの簡単な文に分けることで行えます。\n",
    "最終的な簡略化された文章は、文法的に正しく、流暢で、元の文章の主要な内容を変えずに保持する必要があります。\n",
    "\n",
    "複雑な文章：{complex_sentence}\n",
    "\n",
    "### Assistant: \"\"\"\n",
    "\n",
    "ts_task_description_sl = \"\"\"### Human: Prosim, prepišite naslednji zapleten stavek, da bo lažje razumljiv. \n",
    "To lahko storite tako, da nadomestite zapletene besede s preprostejšimi sopomenkami (t.j. parafraziranje), odstranite nepomembne informacije (t.j. stiskanje) in/ali razdelite dolg in zapleten stavek na več preprostejših stavkov. \n",
    "Končni poenostavljeni stavek mora biti slovnično pravilen, tekoč in ohraniti glavne ideje izvirnika brez spreminjanja njegovega pomena.\n",
    "\n",
    "Zapleten stavek: {complex_sentence}\n",
    "\n",
    "### Assistant: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/user/kew/projects/MultiSim/data/English/ASSET_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/English/ASSET_val.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/English/WikiAuto_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/English/WikiAuto_val.csv\n",
      "Wrote 1371 items to ../data/multisim/en-en.json\n",
      "Wrote 1371 items to ../data/multisim/en-en.json\n",
      "Processing /home/user/kew/projects/MultiSim/data/German/GEOLino Corpus_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/German/GEOLino Corpus_train.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/German/GEOLino Corpus_val.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/German/TextComplexityDE Parallel Corpus_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/German/TextComplexityDE Parallel Corpus_train.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/German/TextComplexityDE Parallel Corpus_val.csv\n",
      "Wrote 1371 items to ../data/multisim/en-de.json\n",
      "Wrote 1371 items to ../data/multisim/de-de.json\n",
      "Processing /home/user/kew/projects/MultiSim/data/French/CLEAR Corpus_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/French/CLEAR Corpus_val.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/French/WikiLargeFR Corpus_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/French/WikiLargeFR Corpus_val.csv\n",
      "Wrote 1371 items to ../data/multisim/en-fr.json\n",
      "Wrote 1371 items to ../data/multisim/fr-fr.json\n",
      "Processing /home/user/kew/projects/MultiSim/data/Italian/AdminIT_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Italian/AdminIT_val.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Italian/PaCCSS-IT Corpus_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Italian/PaCCSS-IT Corpus_val.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Italian/Simpitiki Italian Wikipedia_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Italian/Simpitiki Italian Wikipedia_val.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Italian/Teacher_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Italian/Teacher_val.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Italian/Terence_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Italian/Terence_val.csv\n",
      "Wrote 1371 items to ../data/multisim/en-it.json\n",
      "Wrote 1371 items to ../data/multisim/it-it.json\n",
      "Processing /home/user/kew/projects/MultiSim/data/Russian/RSSE Corpus_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Russian/RSSE Corpus_val.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Russian/RuAdapt Ency_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Russian/RuAdapt Ency_val.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Russian/RuAdapt Fairytales_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Russian/RuAdapt Fairytales_val.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Russian/RuWikiLarge_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Russian/RuWikiLarge_val.csv\n",
      "Wrote 1371 items to ../data/multisim/en-ru.json\n",
      "Wrote 1371 items to ../data/multisim/ru-ru.json\n",
      "Processing /home/user/kew/projects/MultiSim/data/Brazilian Portuguese/PorSimples_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Brazilian Portuguese/PorSimples_train.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Brazilian Portuguese/PorSimples_val.csv\n",
      "Wrote 1371 items to ../data/multisim/en-pt.json\n",
      "Wrote 1371 items to ../data/multisim/pt-pt.json\n",
      "Processing /home/user/kew/projects/MultiSim/data/Slovene/Text Simplification Slovene_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Slovene/Text Simplification Slovene_train.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Slovene/Text Simplification Slovene_val.csv\n",
      "Wrote 939 items to ../data/multisim/en-sl.json\n",
      "Wrote 939 items to ../data/multisim/sl-sl.json\n",
      "Processing /home/user/kew/projects/MultiSim/data/Japanese/Easy Japanese Corpus_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Japanese/Easy Japanese Corpus_val.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Japanese/Easy Japanese Extended_test.csv\n",
      "Processing /home/user/kew/projects/MultiSim/data/Japanese/Easy Japanese Extended_val.csv\n",
      "Wrote 1371 items to ../data/multisim/en-ja.json\n",
      "Wrote 1371 items to ../data/multisim/ja-ja.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# gather all data available into a large dataframe\n",
    "\n",
    "# prepare data from multisim\n",
    "\n",
    "data_path = Path('/home/user/kew/projects/MultiSim/data')\n",
    "output_path = Path('../data/multisim/')\n",
    "output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "langs = {\n",
    "    \"en\": (\"English\", ts_task_description_en),\n",
    "    \"de\": (\"German\", ts_task_description_de),\n",
    "    \"fr\": (\"French\", ts_task_description_fr),\n",
    "    \"it\": (\"Italian\", ts_task_description_it),\n",
    "    \"ru\": (\"Russian\", ts_task_description_ru),\n",
    "    \"pt\": (\"Brazilian Portuguese\", ts_task_description_pt),\n",
    "    \"sl\": (\"Slovene\", ts_task_description_sl),\n",
    "    \"ja\": (\"Japanese\", ts_task_description_ja),\n",
    "    # \"eu\": (\"Basque\", ts_task_description_eu),\n",
    "}\n",
    "\n",
    "for lang in langs:\n",
    "    tgt_dir = data_path / langs[lang][0]\n",
    "\n",
    "    en_outfile = output_path / f'en-{lang}.json'\n",
    "    x_outfile = output_path / f'{lang}-{lang}.json'\n",
    "    \n",
    "    c = 0\n",
    "\n",
    "    items = []\n",
    "    \n",
    "    for dataset_split in sorted(tgt_dir.glob('*.csv')):\n",
    "        if lang not in ['de', 'pt', 'sl'] and '_train' in dataset_split.name:\n",
    "            pass\n",
    "        else:\n",
    "            print(f'Processing {dataset_split}')\n",
    "            df = pd.read_csv(dataset_split)\n",
    "            \n",
    "            # # drop rows that are empty\n",
    "            # for col in df.columns:\n",
    "            #     df = df[df[col].notna()]\n",
    "\n",
    "            # print(f'Loaded {len(df)} rows')\n",
    "            # print(f'Columns: {df.columns}')\n",
    "\n",
    "            # write to json\n",
    "            for idx, row in df.iterrows():\n",
    "                # gather targets from all columns except 'original' provided they are not empty\n",
    "                targets = [row[col].strip() for col in df.columns if col != 'original' and isinstance(row[col], str) and row[col].strip()]\n",
    "\n",
    "                # ensure targets are unique\n",
    "                targets = list(set(targets))\n",
    "\n",
    "                en_item = {\n",
    "                    'instruction': ts_task_description_en.format(complex_sentence=row['original']),\n",
    "                    'source': row['original'],\n",
    "                    'target': targets,\n",
    "                    'id': idx,\n",
    "                    'dataset': dataset_split.name,\n",
    "                }\n",
    "\n",
    "                x_item = {\n",
    "                    'instruction': langs[lang][1].format(complex_sentence=row['original']),\n",
    "                    'source': row['original'],\n",
    "                    'target': targets,\n",
    "                    'id': idx,\n",
    "                    'dataset': dataset_split.name,\n",
    "                }\n",
    "\n",
    "                items.append((en_item, x_item))\n",
    "    \n",
    "    # shuffle items\n",
    "    random.shuffle(items)\n",
    "\n",
    "    # truncate to 1000 items\n",
    "    items = items[:1371]\n",
    "\n",
    "    with open(en_outfile, 'w', encoding='utf8') as en_out_f:\n",
    "        with open(x_outfile, 'w', encoding='utf8') as x_out_f:\n",
    "            for item in items:\n",
    "                en_out_f.write(f'{json.dumps(item[0], ensure_ascii=False)}\\n')\n",
    "                x_out_f.write(f'{json.dumps(item[1], ensure_ascii=False)}\\n')\n",
    "            print(f'Wrote {len(items)} items to {en_outfile}')\n",
    "            print(f'Wrote {len(items)} items to {x_outfile}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare data from multisim\n",
    "\n",
    "# data_path = Path('/home/user/kew/projects/MultiSim/data')\n",
    "# output_path = Path('../data/multisim/')\n",
    "# output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# langs = {\n",
    "#     \"en\": (\"English\", ts_task_description_en),\n",
    "#     \"de\": (\"German\", ts_task_description_de),\n",
    "#     \"fr\": (\"French\", ts_task_description_fr),\n",
    "#     \"it\": (\"Italian\", ts_task_description_it),\n",
    "#     \"ru\": (\"Russian\", ts_task_description_ru),\n",
    "#     \"pt\": (\"Brazilian Portuguese\", ts_task_description_pt),\n",
    "#     \"sl\": (\"Slovene\", ts_task_description_sl),\n",
    "#     \"ja\": (\"Japanese\", ts_task_description_ja),\n",
    "#     # \"eu\": (\"Basque\", ts_task_description_eu),\n",
    "# }\n",
    "\n",
    "# for lang in langs:\n",
    "#     tgt_dir = data_path / langs[lang][0]\n",
    "\n",
    "#     en_outfile = output_path / f'en-{lang}.json'\n",
    "#     x_outfile = output_path / f'{lang}-{lang}.json'\n",
    "    \n",
    "#     c = 0\n",
    "\n",
    "#     items = []\n",
    "    \n",
    "#     for dataset_split in sorted(tgt_dir.glob('*.csv')):\n",
    "#         if '_train' in dataset_split.name:\n",
    "#             pass\n",
    "#         else:\n",
    "#             print(f'Processing {dataset_split}')\n",
    "#             df = pd.read_csv(dataset_split)\n",
    "            \n",
    "#             # # drop rows that are empty\n",
    "#             # for col in df.columns:\n",
    "#             #     df = df[df[col].notna()]\n",
    "\n",
    "#             # print(f'Loaded {len(df)} rows')\n",
    "#             # print(f'Columns: {df.columns}')\n",
    "\n",
    "#             # write to json\n",
    "#             for idx, row in df.iterrows():\n",
    "#                 # gather targets from all columns except 'original' provided they are not empty\n",
    "#                 targets = [row[col].strip() for col in df.columns if col != 'original' and isinstance(row[col], str) and row[col].strip()]\n",
    "\n",
    "#                 # ensure targets are unique\n",
    "#                 targets = list(set(targets))\n",
    "\n",
    "#                 en_item = {\n",
    "#                     'instruction': ts_task_description_en.format(complex_sentence=row['original']),\n",
    "#                     'source': row['original'],\n",
    "#                     'target': targets,\n",
    "#                     'id': idx,\n",
    "#                     'dataset': dataset_split.name,\n",
    "#                 }\n",
    "\n",
    "#                 x_item = {\n",
    "#                     'instruction': langs[lang][1].format(complex_sentence=row['original']),\n",
    "#                     'source': row['original'],\n",
    "#                     'target': targets,\n",
    "#                     'id': idx,\n",
    "#                     'dataset': dataset_split.name,\n",
    "#                 }\n",
    "\n",
    "#                 items.append((en_item, x_item))\n",
    "    \n",
    "#     # shuffle items\n",
    "#     random.shuffle(items)\n",
    "\n",
    "#     # truncate to 1000 items\n",
    "#     items = items[:1000]\n",
    "\n",
    "#     with open(en_outfile, 'w', encoding='utf8') as en_out_f:\n",
    "#         with open(x_outfile, 'w', encoding='utf8') as x_out_f:\n",
    "#             for item in items:\n",
    "#                 en_out_f.write(f'{json.dumps(item[0], ensure_ascii=False)}\\n')\n",
    "#                 x_out_f.write(f'{json.dumps(item[1], ensure_ascii=False)}\\n')\n",
    "#             print(f'Wrote {len(items)} items to {en_outfile}')\n",
    "#             print(f'Wrote {len(items)} items to {x_outfile}')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 472 items to ../data/ts/alector.json\n"
     ]
    }
   ],
   "source": [
    "# alector\n",
    "\n",
    "input_dir = '/home/user/kew/projects/muss_original/resources/datasets/alector'\n",
    "\n",
    "output_file = f'../data/ts/alector.json'\n",
    "Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# read data\n",
    "\n",
    "src_texts = []\n",
    "tgt_texts = []\n",
    "for split in ['test', 'valid']:\n",
    "    with open(f'{input_dir}/{split}.complex') as src_f:\n",
    "        for line in src_f:\n",
    "            src_texts.append(line.strip())\n",
    "    with open(f'{input_dir}/{split}.simple') as tgt_f:\n",
    "        for line in tgt_f:\n",
    "            tgt_texts.append(line.strip())\n",
    "    \n",
    "assert len(src_texts) == len(tgt_texts)\n",
    "\n",
    "# write lines to jsonl\n",
    "c = 0\n",
    "with open(output_file, 'w') as out_f:\n",
    "    for src, tgt in zip(src_texts, tgt_texts):\n",
    "        formatted_item = {\n",
    "            'instruction': ts_task_description_en.format(complex_sentence=src).strip(),\n",
    "            'source': src,\n",
    "            'target': tgt\n",
    "        }\n",
    "        out_f.write(f'{json.dumps(formatted_item, ensure_ascii=False)}\\n')\n",
    "        c += 1\n",
    "print(f'Wrote {c} items to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 273 items to ../data/ts/cefet.json\n"
     ]
    }
   ],
   "source": [
    "input_dir = '/home/user/kew/projects/muss_original/resources/datasets/cefet_small/'\n",
    "\n",
    "output_file = f'../data/ts/cefet.json'\n",
    "Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# read data\n",
    "\n",
    "src_texts = []\n",
    "tgt_texts = []\n",
    "for split in ['test', 'valid']:\n",
    "    with open(f'{input_dir}/{split}.complex') as src_f:\n",
    "        for line in src_f:\n",
    "            src_texts.append(line.strip())\n",
    "    with open(f'{input_dir}/{split}.simple') as tgt_f:\n",
    "        for line in tgt_f:\n",
    "            tgt_texts.append(line.strip())\n",
    "    \n",
    "assert len(src_texts) == len(tgt_texts)\n",
    "\n",
    "# write lines to jsonl\n",
    "c = 0\n",
    "with open(output_file, 'w') as out_f:\n",
    "    for src, tgt in zip(src_texts, tgt_texts):\n",
    "        formatted_item = {\n",
    "            'instruction': ts_task_description_en.format(complex_sentence=src).strip(),\n",
    "            'source': src,\n",
    "            'target': tgt\n",
    "        }\n",
    "        out_f.write(f'{json.dumps(formatted_item, ensure_ascii=False)}\\n')\n",
    "        c += 1\n",
    "print(f'Wrote {c} items to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 249 items to ../data/ts/de19.json\n"
     ]
    }
   ],
   "source": [
    "input_dir = '/home/user/kew/projects/muss_tbd/resources/data/text_complexity_de19'\n",
    "\n",
    "output_file = f'../data/ts/de19.json'\n",
    "Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# read data\n",
    "\n",
    "src_texts = []\n",
    "tgt_texts = []\n",
    "for split in ['test', 'valid']:\n",
    "    with open(f'{input_dir}/{split}.complex') as src_f:\n",
    "        for line in src_f:\n",
    "            src_texts.append(line.strip())\n",
    "    with open(f'{input_dir}/{split}.simple') as tgt_f:\n",
    "        for line in tgt_f:\n",
    "            tgt_texts.append(line.strip())\n",
    "    \n",
    "assert len(src_texts) == len(tgt_texts)\n",
    "\n",
    "# write lines to jsonl\n",
    "c = 0\n",
    "with open(output_file, 'w') as out_f:\n",
    "    for src, tgt in zip(src_texts, tgt_texts):\n",
    "        formatted_item = {\n",
    "            'instruction': ts_task_description_en.format(complex_sentence=src).strip(),\n",
    "            'source': src,\n",
    "            'target': tgt\n",
    "        }\n",
    "        out_f.write(f'{json.dumps(formatted_item, ensure_ascii=False)}\\n')\n",
    "        c += 1\n",
    "print(f'Wrote {c} items to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apa\n",
    "\n",
    "# input_dir = '../data/ts/apa_lha/A2-OR/'\n",
    "\n",
    "# # for file in sorted(Path(input_dir).iterdir()):\n",
    "# #     print(file)\n",
    "\n",
    "# src_files = list(Path(input_dir).glob('*.de'))\n",
    "# print(len(src_files))\n",
    "\n",
    "# for src_file in src_files:\n",
    "#     tgt_file = Path(input_dir) / f'{src_file.stem}_A2.simpde'\n",
    "#     if not tgt_file.exists():\n",
    "#         print(f'No target file for {tgt_file}')\n",
    "#     with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
